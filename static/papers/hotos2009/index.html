<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xml:lang="en" lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  

  <title>FAWNdamentally Power-efficient Clusters</title>
  <meta content="text/html; charset=us-ascii" http-equiv="Content-Type" />
  
  <link href="hotos2009.css" rel="STYLESHEET" type="text/css" />
  <script src="citations.js" type="text/javascript">
</script>
</head>

<body>
  

  <table class="title">
    <tr>
      <td>
        <h1 class="titlemain"><b>FAWNdamentally Power-efficient
        Clusters</b></h1>

        <h3 class="titlerest">Vijay Vasudevan, Jason
        Franklin, David Andersen<br />
        Amar Phanishayee, Lawrence Tan, Michael
        Kaminsky<sup>*</sup>, Iulian Moraru<br />
        <em>Carnegie Mellon University and</em>
        <sup><em>*</em></sup><em>Intel Research,
        Pittsburgh</em></h3>
      </td>
    </tr>
  </table>

  <a href="http://www.azoft.com/people/seremina/edu/fawn-rom.html">Romanian Translation</a>

  <h2 id="htoc1">1&nbsp;&nbsp;Introduction</h2>

  <p><a id="sec:intro"></a> Power is becoming an
  increasingly large financial and scaling burden for computing and
  society. The costs of running large data centers are becoming
  dominated by power and cooling to the degree that companies such
  as Microsoft and Google have built new data centers close to
  large and cost-efficient hydroelectric power
  sources&nbsp;[<a href="#google-dalles2005" class="citation"><cite>8</cite></a>]. Studies have projected
  that by 2012, 3-year data center energy costs will be double that
  of server equipment expenditures&nbsp;[<a href="#Qureshi:hotnets2008" class="citation"><cite>15</cite></a>]. Power consumption
  and related cooling costs have become a primary design constraint
  at all levels, limiting the achievable density of data centers
  and large systems, and pushing processor manufacturers towards
  alternative architectures. While power constraints have pushed
  the processor industry toward multi-core architectures,
  power-efficient alternatives to traditional disk and DRAM-based
  cluster architectures have been slow to emerge.</p>

  <p>As a power-efficient alternative for data-intensive computing,
  we propose a cluster architecture called a <em>Fast Array of
  Wimpy Nodes</em>, or FAWN. A FAWN consists of a large number of
  slower but efficient nodes that each draw only a few watts of
  power, coupled with low-power storage&mdash;our prototype FAWN
  nodes are built from 500MHz embedded devices with CompactFlash
  storage that are typically used as wireless routers, Internet
  gateways, or thin clients.</p>

  <p>Through our preliminary evaluation, we demonstrate that a FAWN
  can be up to six times more efficient than traditional systems
  with Flash storage in terms of <i>queries per joule</i> for
  seek-bound applications and between two to eight times more
  efficient for I/O throughput-bound applications (&sect;<a href="#sec:arch">3</a>).</p>

  <p>Long-lasting, fundamental trends in the scaling of computation
  and energy suggest that the FAWN approach will become dominant
  for increasing classes of workloads. First, as we show in
  &sect;<a href="#sec:trends">4</a>, slower processors are more
  efficient: they use fewer joules of energy per instruction than
  higher speed processors. Second, dynamic power scaling techniques
  are less effective than reducing a cluster's peak power
  consumption. We conclude with an analysis of the design space for
  seek-bound workloads, showing how one should use a FAWN
  architecture for various dataset sizes and desired query rates
  (&sect;<a href="#sec:altarch">5</a>).</p>
  

  <h2 id="htoc2">2&nbsp;&nbsp;Data-intensive Computing</h2>
  

  <p>Data-intensive applications have recently become a focus in
  the systems research community, with a resurgence in interest on
  how to store, retrieve, and process massive amounts of data.
  These data-intensive workloads are often I/O-bound, and can be
  broadly classified into two forms: seek-bound and scan-bound
  workloads.</p>

  <h5 class="paragraph">Seek-bound Workloads:</h5>
   
  <p>Seek-bound workloads are exemplified by read-mostly workloads 
  with random access
  patterns for small objects from a large corpus of data. These
  seek-bound workloads are growing in importance and in popularity
  with existing and emerging Internet applications. Many of these
  applications exist in an environment where stringent response
  time requirements preclude heuristics such as caches or data
  layout clustering.</p>

  <p>The corresponding random seeks generated by these workloads
  are poorly suited to conventional disk-based architectures where
  magnetic hard disks limit performance: Access times for a random
  small block of data on a magnetic disk average 3 to 5 ms,
  providing only 200-300 requests per second per disk.</p>

  <p>Driven by the need to perform millions of random accesses per
  second&nbsp;[<a href="#www-facebook-memcached" class="citation"><cite>5</cite></a>], social networking
  and blogging sites such as LiveJournal and Facebook have already
  been forced to create and maintain large cluster-based memory
  caches such as <tt>memcached</tt>&nbsp;[<a href="#fitzpatrick:lj2005" class="citation"><cite>6</cite></a>]. The same challenges
  are also faced by e-commerce sites such as Amazon, which have
  catalogs of hundreds of millions or more objects, mostly accessed
  by a unique object ID&nbsp;[<a href="#decania:sosp2007" class="citation"><cite>4</cite></a>]. As we show in
  &sect;<a href="#sec:altarch">5</a>, storing this large amount of
  data entirely in DRAM is often more costly than storing the data
  on disk.</p>

  <h5 class="paragraph">Scan-bound
  Workloads:</h5>

  <p>The second class of data-intensive workloads we consider is
  exemplified by large-scale data-analysis. Analysis of large,
  unstructured datasets is becoming increasingly important in
  logfile analysis, data-mining, and for large-data applications
  such as machine learning. Many of these workloads are
  characterized by very simple computations (e.g., word counts or
  <tt>grep</tt>) over the entire dataset. These workloads are, at
  first glance, well suited to platter-based disks, which provide
  fast sequential I/O. In many cases, however, the I/O capability
  provided by a typical drive or small RAID array is insufficient
  to saturate a modern high-speed, high-power CPU. As a result,
  performance is limited by the speed at which the storage system
  can deliver data to the processors.</p>

  <p>As one example of scan-bound workloads under-utilizing CPUs,
  Yahoo won the Terabyte Sort (TeraSort) benchmark in 2008 using a
  Hadoop cluster with nearly 4000 disks, sorting 1TB of records in
  209 seconds&nbsp;[<a href="#www-terasort" class="citation"><cite>18</cite></a>]. Google subsequently
  completed the benchmark 3 times faster, using a similar number
  and type of nodes but with 3 times as many disks&nbsp;[<a href="#www-hamilton-terasort" class="citation"><cite>9</cite></a>].
  These numbers suggest that much of the speed improvement can be
  attributed to improving the rate that data can be delivered to
  the processor from storage, and that the high-speed, high-power
  CPUs used in the first example were under-utilized.</p>
  

  <h2 id="htoc3">3&nbsp;&nbsp;A Fast Array of Wimpy Nodes</h2>
  

  <p><a id="sec:arch"></a> We propose the Fast
  Array of Wimpy Nodes (FAWN) architecture, which uses a large
  number of &ldquo;wimpy&rdquo; nodes that act as data
  storage/retrieval nodes. These nodes use energy-efficient
  low-power processors combined with low-power storage and a small
  amount of DRAM.</p>

  <p>We have explored two preliminary workloads to understand how a
  FAWN system can be constructed. The first, FAWN-SEEK, examines
  exact key-value queries at large scale such as those seen in
  <tt>memcached</tt> and Amazon's Dynamo&nbsp;[<a href="#decania:sosp2007" class="citation"><cite>4</cite></a>]. The second,
  FAWN-SCAN, examines unstructured text mining queries similar to
  those expressed in frameworks such as Hadoop and MapReduce.</p>

  <p>To understand the feasibility of a FAWN architecture, we
  evaluated several candidate node systems in their ability to
  satisfy random key-value lookups and simple scan processing.</p>

  <div class="figure">

    <table cellspacing="6" cellpadding="0">
      <tr>
        <td align="left"><b>System / Storage</b></td>

        <td align="right"><b>QPS</b></td>

        <td align="right"><b>Watts</b></td>

        <td align="right">
        <b>Queries/sec</b>/<b>Watt</b></td>
      </tr>

      <tr>
        <td align="left"><i>Embedded Systems</i></td>

        <td align="right">&nbsp;</td>

        <td align="right">&nbsp;</td>

        <td align="right">&nbsp;</td>
      </tr>

      <tr>
        <td align="left">Alix3c2 / Sandisk(CF)</td>

        <td align="right">1697</td>

        <td align="right">4</td>

        <td align="right">424</td>
      </tr>

      <tr>
        <td align="left">Soekris / Sandisk(CF)</td>

        <td align="right">334</td>

        <td align="right">3.75</td>

        <td align="right">89</td>
      </tr>

      <tr>
        <td align="left"><i>Traditional Systems</i></td>

        <td align="right">&nbsp;</td>

        <td align="right">&nbsp;</td>

        <td align="right">&nbsp;</td>
      </tr>

      <tr>
        <td align="left">Desktop / Mobi(SSD)</td>

        <td align="right">5800</td>

        <td align="right">83</td>

        <td align="right">69.9</td>
      </tr>

      <tr>
        <td align="left">MacbookPro / HD</td>

        <td align="right">66</td>

        <td align="right">29</td>

        <td align="right">2.3</td>
      </tr>

      <tr>
        <td align="left">Desktop / HD</td>

        <td align="right">171</td>

        <td align="right">87</td>

        <td align="right">1.96</td>
      </tr>
    </table>

    <p class="caption">

      Table 1: <a id="table:randqueries"></a>Query
          rates and power costs using different machine
          configurations. Power measured using a WattsUp meter
          (<a href="http://wattsupmeters.com"><tt>http://wattsupmeters.com</tt></a>).
    
</p>

  
</div>


  <p><b>FAWN-SEEK Performance:</b> Table&nbsp;<a href="#table:randqueries" class="citation">1</a> shows the rate at which these
  nodes could service requests for random keys from an on-disk
  dataset, via the network. The best embedded system (Alix3c2)
  using CompactFlash (CF) storage was six times more
  power-efficient (in queries/joule) than even the low-power
  desktop node with a modern SATA-based Flash device.</p>

  <p><b>FAWN-SCAN Performance:</b> We have performed two
  preliminary studies of scan-bound workloads. First, we look at
  the distributed sort benchmark provided by the Hadoop framework.
  Sorting 1&nbsp;GB of data on one Alix3c2 node took 160 seconds
  while consuming only 4&nbsp;W, providing a sort efficiency of
  1.6MB per joule. In contrast, a traditional desktop machine with
  magnetic disk required only 53 seconds but consumed 130&nbsp;W, a
  sort efficiency of 0.2MB per joule&mdash;eight times less
  efficient.</p>

  <p>Next, we examine a workload derived from a machine learning
  application that takes a massive-data approach to
  semi-supervised, automated learning of word classification. The
  problem reduces to counting the number of times each phrase, from
  a set of thousands to millions of phrases, occurs in a massive
  corpus of sentences extracted from the Web. Our results are
  promising but challenging. FAWN converts a formerly I/O-bound
  problem into a CPU-bound problem, which requires great
  algorithmic and implementation attention to work well. The
  Alix3c2 wimpies can <tt>grep</tt> for a single pattern at
  25MB/sec, close to the maximum rate the CF can provide. However,
  searching for thousands or millions of phrases with the naive
  Aho-Corasick algorithm in <tt>grep</tt> becomes memory-bound (and
  exceeds the capacity of the wimpy nodes, with unpleasant
  results).</p>

  <p>We have optimized this search using a rolling hash function
  and large bloom filter to provide a one-sided error grep (false
  positive but no false negatives) that achieves roughly twice the
  power efficiency (bytes per second per watt) as a conventional
  node. However, this efficiency came at the cost of considerable
  implementation effort. Our experience suggests that efficiently
  using wimpy nodes for some scan-based workloads will require the
  development of easy-to-use frameworks that provide common,
  heavily-optimized data reduction operations (e.g., grep,
  multi-word grep, etc) as primitives. This represents an exciting
  avenue of future work: while speeding up hardware is difficult,
  programmers have long excelled at finding ways to optimize
  CPU-bound problems.</p>

  <h2 id="htoc4">4&nbsp;&nbsp;Why FAWN?</h2>

  <p><a id="sec:trends"></a> A FAWN's maximum
  power consumption is many times lower than a modern DRAM and
  disk-based cluster while serving identical workloads. Several
  trends in power and scaling suggest that the FAWN approach will
  continue to be an energy-efficient approach to building clusters
  for years to come.</p>
  

  <h5 class="paragraph">1. The Increasing CPU-I/O
  Gap.</h5>

  <p>One constant and consternating trend over the last few decades
  is the increasing gap between CPU performance and I/O bandwidth.
  The &ldquo;memory wall&rdquo; remains a challenge in scaling
  performance with both CPU frequency and with increasing core
  counts&nbsp;[<a href="#Murphy:iiswc2007" class="citation"><cite>13</cite></a>]. For data-intensive
  computing workloads, storage, network, and memory bandwidth
  bottlenecks often cause low CPU utilization.</p>

  <p>To efficiently run I/O-bound data-intensive, computationally
  simple applications, FAWN uses wimpy processors selected to
  reduce I/O-induced idle cycles while maintaining high
  performance. The reduced processor speed then benefits from a
  second trend:</p>


  

  <h5 class="paragraph">2. CPU power consumption
  grows faster than speed.</h5>

  <p>Techniques to mask the CPU-memory bottleneck come at the cost
  of energy efficiency. Branch prediction, speculative execution,
  and increasing the amount of on-chip caching all require
  additional processor die area; modern processors dedicate as much
  as half their die to L2/3 caches&nbsp;[<a href="#www-intel-penryn" class="citation"><cite>11</cite></a>]. These techniques do not
  increase the speed of basic computations, but do increase power
  consumption, making faster CPUs less energy efficient.</p>

  <p>A FAWN cluster's slower CPUs dedicate more transistors to
  basic operations. These CPUs execute significantly more
  <em>instructions per joule</em> (or instructions/sec per Watt)
  than their faster counterparts (Figure&nbsp;<a href="#fig:inst_power">1</a>). For example, a Xeon 7350 operates 4
  cores at 2.66GHz, consuming about 80W. Assuming optimal pipeline
  performance (4 operations per cycle), this processor
  optimistically operates at 530M instructions/joule&mdash;if it
  can issue enough instructions per clock cycle and does not stall
  or mispredict. A single-core XScale StrongARM processor running
  at 800MHz consumes 0.5W, providing 1600M instructions per joule.
  The performance-to-power ratio of the XScale is three times that
  of the Xeon, and is likely higher given real-world pipeline
  performance for data-intensive applications. Tellingly, this
  difference is more severe when considered as I/O operations per
  joule: the XScale has a 260MHz memory bus; the Xeon has a 1GHz
  memory bus, five times faster but at 160 times the power.</p>

  <div class="figure">

    <img src="images/inst_power_color.png" alt="Instruction Efficiency"/>

    <p class="caption">

      Figure 1: <a id="fig:inst_power"></a> Max speed
          (MIPS) vs. Instruction efficiency (MIPS/W) in log-log
          scale. Numbers gathered from publicly-available spec
          sheets and manufacturer product websites.
    
</p>

  
</div>
  

  <h5 class="paragraph">3. Dynamic power scaling
  on traditional systems is surprisingly ineffective.</h5>
  

  <p>&ldquo;Energy-proportional&rdquo; systems attempt to ensure
  that systems dynamically scale back power usage with decreasing
  load (e.g., operating at 20% utilization should use 20% of peak
  power). We argue that reducing peak power is more effective than
  dynamic power scaling for several reasons.</p>

  <p>First, dynamic voltage and frequency scaling (DVFS) benefits
  for CPUs are now quite limited. A primary energy-saving benefit
  of DVFS was its ability to reduce voltage as it reduced
  frequency, but modern CPUs already operate near the voltage
  floor. Second, transistor leakage currents quickly become a
  dominant power cost, which drops much more slowly as frequency is
  reduced&nbsp;[<a href="#deLangen:lcns" class="citation"><cite>3</cite></a>].</p>

  <p>Second, non-processor components have begun to dominate energy
  consumption in data centers&nbsp;[<a href="#Barroso:Computer" class="citation"><cite>1</cite></a>], requiring that all
  components be scaled back with demand, including device
  controllers and power supplies. Despite improvements to power
  scaling technology, systems remain most power-efficient when
  operating at peak power&nbsp;[<a href="#Tolia:hotpower2008" class="citation"><cite>19</cite></a>]. Given the difficulty
  of scaling all system components, we must therefore consider
  &ldquo;constant factors&rdquo; for power when calculating a
  system's instruction efficiency. Figure&nbsp;<a href="#fig:inst_power_fixed">2</a> plots processor efficiency when
  adding a fixed 0.1W cost for system components such as Ethernet.
  Because powering 10Mbps Ethernet dwarfs the power consumption of
  the tiny sensor-type processors that consume only micro-Watts of
  power, their efficiency drops significantly. The best operating
  point exists in the middle of the curve, where the fixed costs
  are amortized while still providing
  energy-efficiency.</p>

  <div class="figure">

    <p class="fig"><img src="images/inst_power_eth_color.png" alt="Instruction Efficiency with 0.1W system overhead" /></p>


    <p class="caption">

      Figure 2: <a id="fig:inst_power_fixed"></a>
          Processor efficiency when adding fixed 0.1W system
          overhead.
    
</p>

  
</div>


  <p>Newer techniques aim for energy proportionality by turning
  machines off and using VM consolidation, but the practicality of
  these techniques is still being explored. Many large-scale
  systems often operate below 50% utilization, but opportunities to
  go into deep sleep states are few and far between&nbsp;[<a href="#Barroso:Computer" class="citation"><cite>1</cite></a>], while
  &ldquo;wake-up&rdquo; or VM migration penalties can make these
  techniques less energy-efficient.  Also,
  VM migration may not apply for some applications, e.g., if datasets 
  are held entirely in DRAM to guarantee fast response times.</p>

  <p>Even if techniques for dynamically scaling below peak power
  were effective, operating below peak power capacity has one more
  drawback:</p>
  

  <h5 class="paragraph">4. Peak power consumption
  limits data center density.</h5>

  <p>Data centers must be provisioned for a system's maximum power
  draw. This requires investment in infrastructure, including
  worst-case cooling requirements, provisioning of batteries for
  backup systems on power failure, and proper gauge power cables.
  FAWN significantly reduces maximum power draw in comparison to
  traditional cluster systems that provide equivalent performance, 
  thereby reducing infrastructure
  cost, reducing the need for massive overprovisioning, and
  removing one limit to the achievable density of data centers.</p>

  <p>Finally, energy proportionality alone is not a panacea:
  systems ideally should be both proportional <em>and</em>
  efficient at 100% load. In this paper, we show that there is
  significant room to improve energy efficiency, and the FAWN
  approach provides a simple way to do so.</p>
  

  <h2 id="htoc5">5&nbsp;&nbsp;Alternatives: <em>When</em> FAWN?</h2>
  

  <p><a id="sec:altarch"></a> Next, we address
  when one should use a FAWN or a traditional system by estimating
  the three-year total cost of ownership for a cluster serving a
  seek-bound workload.</p>

  <p>A cluster needs enough nodes to both hold the entire dataset
  and to serve a particular query rate. For a dataset of <i>DS</i>
  gigabytes and a query rate <i>QR</i>, the number of nodes in a
  cluster is:</p>

  <table class="display dcenter">
    <tr valign="middle">
      <td class="dcell">
        <table cellspacing="6" cellpadding="0">
          <tr>
            <td align="right">
              <table class="display">
                <tr valign="middle">
                  <td class="dcell">
                  &nbsp;<i>N</i>&nbsp;=&nbsp;<i>max</i>(</td>

                  <td class="dcell">
                    <table class="display">
                      <tr>
                        <td class="dcell" align="center">
                        <i>DS</i></td>
                      </tr>

                      <tr>
                        <td class="hbar"></td>
                      </tr>

                      <tr>
                        <td class="dcell" align="center">
                          <table class="display">
                            <tr>
                              <td class="dcellsmall" align="center">
                              <i>gb</i></td>
                            </tr>

                            <tr>
                              <td class="hbar"></td>
                            </tr>

                            <tr>
                              <td class="dcellsmall" align="center">
                              <i>node</i></td>
                            </tr>
                          </table>
                        </td>
                      </tr>
                    </table>
                  </td>

                  <td class="dcell">,&nbsp;</td>

                  <td class="dcell">
                    <table class="display">
                      <tr>
                        <td class="dcell" align="center">
                        <i>QR</i></td>
                      </tr>

                      <tr>
                        <td class="hbar"></td>
                      </tr>

                      <tr>
                        <td class="dcell" align="center">
                          <table class="display">
                            <tr>
                              <td class="dcellsmall" align="center">
                              <i>qr</i></td>
                            </tr>

                            <tr>
                              <td class="hbar"></td>
                            </tr>

                            <tr>
                              <td class="dcellsmall" align="center">
                              <i>node</i></td>
                            </tr>
                          </table>
                        </td>
                      </tr>
                    </table>
                  </td>

                  <td class="dcell">)</td>
                </tr>
              </table>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>

  <p>We define the 3-year total cost of ownership (TCO) for an
  individual node as the sum of the capital cost and the 3-year
  power cost at 10 cents per kWh.</p>

  <p>If a cluster's TCO grows linearly with the number of nodes,
  the <em>ratio</em> of dataset size to query rate informs the
  relative importance between storage size and query rate. For
  large dataset size to query rate ratios, the number of nodes
  required is dominated by the storage capacity per node: the
  important metric is the total cost per GB for an individual node.
  Conversely, for small datasets with high query rates, the
  per-node query capacity dictates the number of nodes: the
  dominant metric is queries per second per dollar. Between these
  extremes, systems must provide the best tradeoff between per-node
  storage capacity, query rate, and power cost.</p>

  <p>To better understand these tradeoffs, we provide statistics
  for several candidate systems in Table&nbsp;<a href="#table:alternatives" class="citation">2</a>; we choose a cluster composed of
  traditional nodes (base power of 200W and base cost of $1000) or
  a FAWN system with wimpy nodes (base power of 5W, $150), pairing
  each node with several storage solutions. Costs for traditional
  machines were calculated based on real quotes obtained when
  building an 80-node traditional cloud computing cluster, and then
  dividing by a factor of two to account for even larger bulk
  discounts; costs for FAWN machines were based on private
  communication with a large manufacturer. These prices exclude the
  significant costs of power and cooling infrastructure, further
  biasing against FAWN nodes, which require significantly less
  power and cooling for a given performance or storage
  requirement.</p>

  <p><em>Traditional+Disk</em> pairs a single server with five 2TB
  high-speed disks capable of 300 queries/sec. Each disk consumes
  10W. <em>Traditional+SSD</em> uses two Fusion-IO 80GB Flash SSDs,
  each also consuming about 10W. <em>Traditional+DRAM</em> uses
  eight 8GB server-quality DRAM modules, each consuming 10W.</p>

  <p><em>FAWN+Disk</em> nodes use one 2TB 7200RPM disk: we assume
  wimpy nodes have fewer connectors available on the board.
  <em>FAWN+SSD</em> uses one 32GB Intel SATA Flash SSD, consuming
  2W. <em>FAWN+DRAM</em> uses a single 2GB, slower DRAM module,
  also consuming 2W.</p>

  <div class="figure">

    <table cellspacing="6" cellpadding="0">
      <tr>
        <td align="left"><b>System</b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b>Cost</b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b>W</b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b>QPS</b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b><i>Queries</i></b>/<b><i>Joule</i></b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b><i>GB</i></b>/<b><i>Watt</i></b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b><i>TCO</i></b>/<b><i>GB</i></b></td>

        <td align="center" valign="top"></td>

        <td align="right"><b><i>TCO</i></b>/<b><i>QPS</i></b></td>
      </tr>

      <tr>
        <td align="left"><i>Traditionals:</i></td>

        <td align="center" valign="top"></td>
      </tr>

      <tr>
        <td align="left">5-2TB HD</td>

        <td align="center" valign="top"></td>

        <td align="right">$2K</td>

        <td align="center" valign="top"></td>

        <td align="right">250</td>

        <td align="center" valign="top"></td>

        <td align="right">1500</td>

        <td align="center" valign="top"></td>

        <td align="right">6</td>

        <td align="center" valign="top"></td>

        <td align="right">40</td>

        <td align="center" valign="top"></td>

        <td align="right">0.26</td>

        <td align="center" valign="top"></td>

        <td align="right">1.77</td>
      </tr>

      <tr>
        <td align="left">160GB PCIe
        SSD</td>

        <td align="center" valign="top"></td>

        <td align="right">$8K</td>

        <td align="center" valign="top"></td>

        <td align="right">220</td>

        <td align="center" valign="top"></td>

        <td align="right">200K</td>

        <td align="center" valign="top"></td>

        <td align="right">909</td>

        <td align="center" valign="top"></td>

        <td align="right">0.7</td>

        <td align="center" valign="top"></td>

        <td align="right">53</td>

        <td align="center" valign="top"></td>

        <td align="right">0.04</td>
      </tr>

      <tr>
        <td align="left">64GB
        DRAM</td>

        <td align="center" valign="top"></td>

        <td align="right">$3K</td>

        <td align="center" valign="top"></td>

        <td align="right">280</td>

        <td align="center" valign="top"></td>

        <td align="right">1M</td>

        <td align="center" valign="top"></td>

        <td align="right">3.5K</td>

        <td align="center" valign="top"></td>

        <td align="right">0.2</td>

        <td align="center" valign="top"></td>

        <td align="right">58</td>

        <td align="center" valign="top"></td>

        <td align="right">0.004</td>
      </tr>

      <tr>
        <td align="left"><i>FAWNs:</i></td>

        <td align="center" valign="top"></td>
      </tr>

      <tr>
        <td align="left">2TB Disk</td>

        <td align="center" valign="top"></td>

        <td align="right">$350</td>

        <td align="center" valign="top"></td>

        <td align="right">15</td>

        <td align="center" valign="top"></td>

        <td align="right">250</td>

        <td align="center" valign="top"></td>

        <td align="right">16</td>

        <td align="center" valign="top"></td>

        <td align="right">133</td>

        <td align="center" valign="top"></td>

        <td align="right">0.19</td>

        <td align="center" valign="top"></td>

        <td align="right">1.56</td>
      </tr>

      <tr>
        <td align="left">32GB SSD</td>

        <td align="center" valign="top"></td>

        <td align="right">$550</td>

        <td align="center" valign="top"></td>

        <td align="right">7</td>

        <td align="center" valign="top"></td>

        <td align="right">35K</td>

        <td align="center" valign="top"></td>

        <td align="right">5K</td>

        <td align="center" valign="top"></td>

        <td align="right">4.6</td>

        <td align="center" valign="top"></td>

        <td align="right">17.8</td>

        <td align="center" valign="top"></td>

        <td align="right">0.016</td>
      </tr>

      <tr>
        <td align="left">2GB DRAM</td>

        <td align="center" valign="top"></td>

        <td align="right">$250</td>

        <td align="center" valign="top"></td>

        <td align="right">7</td>

        <td align="center" valign="top"></td>

        <td align="right">100K</td>

        <td align="center" valign="top"></td>

        <td align="right">14K</td>

        <td align="center" valign="top"></td>

        <td align="right">0.3</td>

        <td align="center" valign="top"></td>

        <td align="right">134</td>

        <td align="center" valign="top"></td>

        <td align="right">0.003</td>
      </tr>
    </table>

    <p class="caption">

      Table
          2: <a id="table:alternatives"></a>Traditional and
          FAWN node statistics
    
</p>

  
</div>


  <p>Figure&nbsp;<a href="#fig:soln_space">3</a> shows which base
  system has the lowest cost for a particular dataset size and
  query rate, with dataset sizes between 100GB and 10PB and query
  rates between 100K and 1 billion per second. The dividing lines
  represent a boundary across which one system becomes more
  favorable than another.</p>

  <p>For large dataset to query rate ratios, <em>FAWN+Disk</em>
  provides the lowest TCO because it has the lowest total cost per
  GB. Intriguingly, if they can be configured with sufficient disks
  per node (over 50), a traditional system + disks wins for
  exabyte-sized workloads with low query rates, though this
  disappears off of our graph (and packing 50 disks per machine may
  harm reliability).</p>

  <p>For small dataset to query rate ratios, <em>FAWN+DRAM</em>
  costs the fewest dollars per queries/second, keeping in mind that
  we do <em>not</em> examine workloads that fit entirely in L2
  cache on a traditional node. This observation is similar to that
  made much earlier by the intelligent RAM project, which coupled
  processors and DRAM to achieve similar benefits&nbsp;[<a href="#Bowman:iram1997" class="citation"><cite>2</cite></a>]. The wimpy
  nodes can only accept 2GB of DRAM per node, so for larger
  datasets, a traditional DRAM system provides a high query rate
  and requires fewer nodes to store the same amount of data (64GB
  vs 2GB per node). Wimpies designed to address 8GB or more of DRAM
  would equalize this ratio, making <em>FAWN+DRAM</em> strictly
  superior to <em>Traditional+DRAM</em> for these random-read
  workloads.</p>

  <p>In the middle range, <em>FAWN+SSD</em>s provide the best
  balance of storage capacity, query rate, and total cost. As SSD
  capacity improves, this combination is likely to continue
  expanding into the range served by <em>FAWN+Disk</em>; as SSD
  performance improves, so will it reach into DRAM territory. It is
  conceivable that <em>FAWN+SSD</em> could become the dominant
  architecture for a wide range of workloads, relegating the other
  architectures to niches for huge but infrequently-accessed
  datasets, or tiny datasets with enormous query rates.</p>

  <p><i>Are traditional systems obsolete?</i> We emphasize that
  this analysis applies only to small, random access workloads.
  Sequential-read workloads are similar, but the constants depend
  strongly on the per-byte processing required. Traditional cluster
  architectures retain a place for CPU-bound workloads, but we do
  note that architectures such as IBM's BlueGene successfully apply
  large numbers of low-power, efficient processors to many
  supercomputing applications&nbsp;[<a href="#Gara:bluegene2005" class="citation"><cite>7</cite></a>]&mdash;but they augment
  their wimpy processors with custom floating point units and very
  low-latency, high-bandwidth interconnects to do so.</p>

  <div class="figure">

    <table cellspacing="6" cellpadding="0">
      <tr>
        <td align="right"><img src="images/soln_space_color.png" alt="Solution Space for seek-bound workloads"/></td>
      </tr>
    </table>

    <p class="caption">

      Figure 3: <a id="fig:soln_space"></a> Solution space
          for lowest 3-year TCO as a function of dataset size and
          query rate.
    
</p>

  
</div>


  <h2 id="htoc6">6&nbsp;&nbsp;Conclusion</h2>

  <p><a id="sec:concl"></a></p>

  <p>The architectural motivation for FAWN borrows from a long line
  of work on balanced computing&nbsp;[<a href="#Rivoire:joulesort" class="citation"><cite>17</cite></a>], multi-microprocessor
  systems for image processing&nbsp;[<a href="#Kushner:zmob" class="citation"><cite>12</cite></a>],
  supercomputing&nbsp;[<a href="#Gara:bluegene2005" class="citation"><cite>7</cite></a>], placing smarts near
  storage&nbsp;[<a href="#Huston:fast2004" class="citation"><cite>10</cite></a><cite>,</cite> <a href="#riedel:activedisk2001" class="citation"><cite>16</cite></a><cite>,</cite>
  <a href="#Bowman:iram1997" class="citation"><cite>2</cite></a>],
  and creating arrays of cheap components to leverage
  &ldquo;dis-efficiencies&rdquo; of scale&nbsp;[<a href="#RAID" class="citation"><cite>14</cite></a>]. FAWN takes advantage of a
  sweet spot in processor efficiency for I/O intensive workloads
  that do not saturate modern CPUs, greatly reducing both average
  and maximum power consumption. Given the increasing importance of
  data-intensive and power-efficient computing, we believe that
  FAWN is the right approach for losing watts fast.</p>
  

  <h2>Acknowledgments</h2>
  

  <p>We would like to thank the anonymous reviewers, Charlie Garrod
  and George Nychis for their valuable feedback and
  suggestions.</p>

  <h2>References</h2>
  

  <dl class="refs">
    <dt id="Barroso:Computer">[1]</dt>

    <dd>Luiz&nbsp;Andr&eacute; Barroso
    and Urs H&ouml;lzle. The case for energy-proportional
    computing. <em>Computer</em>, 40(12):33&ndash;37, 2007.</dd>

    <dt id="Bowman:iram1997">[2]</dt>

    <dd>W.&nbsp;Bowman,
    N.&nbsp;Cardwell, C.&nbsp;Kozyrakis, C.&nbsp;Romer, and
    H.&nbsp;Wang. Evaluation of existing architectures in IRAM
    systems. In <em>Workshop on Mixing Logic and</em>
    <em>DRAM</em><em>, 24th International Symposium on Computer
    Architecture</em>, June 1997.</dd>

    <dt id="deLangen:lcns">[3]</dt>

    <dd>Pepijn de&nbsp;Langen and Ben
    Juurlink. Trade-offs between voltage scaling and processor
    shutdown for low-energy embedded multiprocessors. In
    <em>Embedded Computer Systems: Architectures, Modeling, and
    Simulation</em>, 2007.</dd>

    <dt id="decania:sosp2007">[4]</dt>

    <dd>Guiseppe DeCandia, Deinz
    Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash
    Lakshman, Alex Pilchin, Swami Sivasubramanian, Peter Vosshall,
    and Werner Vogels. Dynamo: Amazon's highly available key-value
    store. In <em>Proc. 21st ACM Symposium on Operating Systems
    Principles (SOSP)</em>, Stevenson, WA, October 2007.</dd>

    <dt id="www-facebook-memcached">[5]</dt>

    <dd>Scaling memcached at Facebook.
    <a href="http://www.facebook.com/note.php?note_id=39391378919"><tt>http://www.facebook.com/note.php?note_id=39391378919</tt></a>.</dd>

    <dt id="fitzpatrick:lj2005">[6]</dt>

    <dd>Brad Fitzpatrick. LiveJournal's
    backend: A history of scaling. Presentation, <a href="http://www.slideshare.net/vishnu/livejournals-backend-a-history-of-scaling/">
    <tt>http://www.slideshare.net/vishnu/livejournals-backend-a-history-of-scaling/</tt></a>, August 2005.</dd>

    <dt id="Gara:bluegene2005">[7]</dt>

    <dd>A.&nbsp;Gara, M.&nbsp;A.
    Blumrich, D.&nbsp;Chen, G&nbsp;L-T Chiu, et&nbsp;al. Overview
    of the Blue Gene/L system architecture. <em>IBM J.</em> <em>Res
    and Dev.</em>, 49(2/3), May 2005.</dd>

    <dt id="google-dalles2005">[8]</dt>

    <dd>Kathy Gray. Port deal with
    Google to create jobs. The Dalles Chronicle, <a href="http://www.gorgebusiness.com/2005/google.htm"><tt>http://www.gorgebusiness.com/2005/google.htm</tt></a>,
    February 2005.</dd>

    <dt id="www-hamilton-terasort">[9]</dt>

    <dd>James Hamilton. Google map
    reduce wins TeraSort. <a href="http://perspectives.mvdirona.com/2008/11/22/GoogleMapReduceWinsTeraSort.aspx">
    <tt>http://perspectives.mvdirona.com/2008/11/22/GoogleMapReduceWinsTeraSort.aspx</tt></a>, 2008.</dd>

    <dt id="Huston:fast2004">[10]</dt>

    <dd>L.&nbsp;Huston,
    R.&nbsp;Sukthankar, R.&nbsp;Wickremesinghe,
    M.&nbsp;Satyanarayanan, G.&nbsp;Ganger, E.&nbsp;Riedel, and
    A.&nbsp;Ailamaki. Diamond: A storage architecture for early
    discard in interactive search. In <em>Proc. 3rd USENIX
    Conference on File and Storage Technologies</em>, San
    Francisco, CA, March 2004.</dd>

    <dt id="www-intel-penryn">[11]</dt>

    <dd>Penryn Press Release. <a href="http://www.intel.com/pressroom/archive/releases/20070328fact.htm">
    <tt>http://www.intel.com/pressroom/archive/releases/20070328fact.htm</tt></a>.</dd>

    <dt id="Kushner:zmob">[12]</dt>

    <dd>T.&nbsp;Kushner, A.&nbsp;Y. Wu,
    and A.&nbsp;Rosenfeld. Image Processing on ZMOB. <em>IEEE
    Trans. Comupters</em>, 31(10), 1982.</dd>

    <dt id="Murphy:iiswc2007">[13]</dt>

    <dd>Richard Murphy. On the effects
    of memory latency and bandwidth on supercomputer application
    performance. In <em>IEEE IISWC</em>, 2007.</dd>

    <dt id="RAID">[14]</dt>

    <dd>David&nbsp;A. Patterson, Garth
    Gibson, and Randy&nbsp;H. Katz. A case for redundant arrays of
    inexpensive disks (RAID). In <em>Proc. ACM SIGMOD</em>,
    Chicago, IL, 1988.</dd>

    <dt id="Qureshi:hotnets2008">[15]</dt>

    <dd>Asfandyar Qureshi. Plugging into
    energy market diversity. In <em>Proc. 7th ACM Workshop on Hot
    Topics in Networks (Hotnets-VII)</em>, Calgary, Alberta.
    Canada., October 2008.</dd>

    <dt id="riedel:activedisk2001">[16]</dt>

    <dd>Erik Riedel, Christos Faloutsos,
    Garth&nbsp;A. Gibson, and David Nagle. Active disks for
    large-scale data processing. <em>IEEE Computer</em>,
    34(6):68&ndash;74, June 2001.</dd>

    <dt id="Rivoire:joulesort">[17]</dt>

    <dd>Suzanne Rivoire, Mehul&nbsp;A.
    Shah, Parthasarathy Ranganathan, and Christos Kozyrakis.
    JouleSort: A balanced energy-efficient benchmark. In <em>Proc.
    ACM SIGMOD</em>, Beijing, China, June 2007.</dd>

    <dt id="www-terasort">[18]</dt>

    <dd>Sort benchmark home page.
    <a href="http://www.hpl.hp.com/hosted/sortbenchmark"><tt>http://www.hpl.hp.com/hosted/sortbenchmark</tt></a>.</dd>

    <dt id="Tolia:hotpower2008">[19]</dt>

    <dd>Niraj Tolia, Zhikui Wang, Manish
    Marwah, Cullen Bash, Parthasarathy Ranganathan, and Xiaoyun
    Zhu. Delivering energy proportionality with non
    energy-proportional systems &ndash; optimizing the ensemble. In
    <em>Proc.</em> <em>HotPower</em>, San Diego, CA, December
    2008.</dd>
  </dl>
  
  

  
</body>
</html>
